{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alanazi433/GenAI/blob/main/GenAI/HW5/Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEzF5CS0dNnF",
        "outputId": "223e41d9-535a-416d-c989-bed1fee391ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import re\n",
        "import string\n"
      ],
      "metadata": {
        "id": "ShH4-Wh_f-yg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Download and combine text data\n",
        "urls = [\n",
        "\"https://www.gutenberg.org/files/1041/1041-0.txt\",  # Hamlet\n",
        "    \"https://www.gutenberg.org/files/152/152-0.txt\",    # Macbeth\n",
        "    \"https://www.gutenberg.org/files/1112/1112-0.txt\",  # Othello\n",
        "    \"https://www.gutenberg.org/files/2265/2265-0.txt\",  # A Midsummer Night's Dream\n",
        "    \"https://www.gutenberg.org/files/1787/1787-0.txt\",  # King Lear\n",
        "    \"https://www.gutenberg.org/files/2264/2264-0.txt\"   # The Tempest\n",
        "]\n",
        "\n",
        "all_text = \"\"\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    all_text += response.text + \"\\n\\n\"\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "all_text = clean_text(all_text)\n",
        "\n",
        "# Tokenize and create sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([all_text])\n",
        "sequences = tokenizer.texts_to_sequences([all_text])[0]\n",
        "sequence_length = 100\n",
        "\n",
        "# Create input (X) and target (y) sequences\n",
        "sequences = [sequences[i:i + sequence_length + 1] for i in range(len(sequences) - sequence_length)]\n",
        "X, y = [], []\n",
        "for seq in sequences:\n",
        "    X.append(seq[:-1])\n",
        "    y.append(seq[-1])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n"
      ],
      "metadata": {
        "id": "lFEjoVvOdZLE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(num_layers, units, epochs=70):\n",
        "    inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "    x = layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "\n",
        "    # Add LSTM layers with dropout for regularization\n",
        "    for _ in range(num_layers - 1):\n",
        "        x = layers.LSTM(units, return_sequences=True, dropout=0.4)(x)\n",
        "    x = layers.LSTM(units, dropout=0.4)(x)\n",
        "\n",
        "    # Layer normalization\n",
        "    x = layers.LayerNormalization()(x)\n",
        "\n",
        "    outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "    model = models.Model(inputs, outputs)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Learning rate scheduler for smoother convergence\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    # Train with modified batch size or increased sequence length\n",
        "    history = model.fit(X, y, batch_size=128, epochs=epochs, callbacks=[lr_scheduler])\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Configurations for models with different layers and units\n",
        "configurations = [\n",
        "    {\"num_layers\": 1, \"units\": 64},\n",
        "    {\"num_layers\": 1, \"units\": 128},\n",
        "    {\"num_layers\": 1, \"units\": 256},\n",
        "    {\"num_layers\": 2, \"units\": 64},\n",
        "    {\"num_layers\": 2, \"units\": 128},\n",
        "    {\"num_layers\": 2, \"units\": 256},\n",
        "    {\"num_layers\": 3, \"units\": 64},\n",
        "    {\"num_layers\": 3, \"units\": 128},\n",
        "    {\"num_layers\": 3, \"units\": 256}\n",
        "]\n",
        "\n",
        "models_histories = []\n",
        "for config in configurations:\n",
        "    print(f\"\\nTraining model with {config['num_layers']} LSTM layers and {config['units']} units per layer...\")\n",
        "    model, history = train_and_evaluate_model(config['num_layers'], config['units'])\n",
        "    models_histories.append((f\"{config['num_layers']} layers, {config['units']} units\", model, history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tYTzOYEdcRT",
        "outputId": "e447fb20-5301-4542-8476-b90989a49689"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with 1 LSTM layers and 64 units per layer...\n",
            "Epoch 1/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.0207 - loss: 7.5986 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.0402 - loss: 6.7041 - learning_rate: 0.0010\n",
            "Epoch 3/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.0803 - loss: 6.1812 - learning_rate: 0.0010\n",
            "Epoch 4/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.1150 - loss: 5.6305 - learning_rate: 0.0010\n",
            "Epoch 5/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.1536 - loss: 5.0703 - learning_rate: 0.0010\n",
            "Epoch 6/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.1954 - loss: 4.5576 - learning_rate: 0.0010\n",
            "Epoch 7/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2391 - loss: 4.0976 - learning_rate: 0.0010\n",
            "Epoch 8/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.2929 - loss: 3.6523 - learning_rate: 0.0010\n",
            "Epoch 9/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3517 - loss: 3.2366 - learning_rate: 0.0010\n",
            "Epoch 10/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.4054 - loss: 2.8843 - learning_rate: 0.0010\n",
            "Epoch 11/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.4596 - loss: 2.5566 - learning_rate: 0.0010\n",
            "Epoch 12/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.4906 - loss: 2.3421 - learning_rate: 0.0010\n",
            "Epoch 13/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.5292 - loss: 2.1384 - learning_rate: 0.0010\n",
            "Epoch 14/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5489 - loss: 1.9852 - learning_rate: 0.0010\n",
            "Epoch 15/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5681 - loss: 1.8703 - learning_rate: 0.0010\n",
            "Epoch 16/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5896 - loss: 1.7356 - learning_rate: 0.0010\n",
            "Epoch 17/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5997 - loss: 1.6664 - learning_rate: 0.0010\n",
            "Epoch 18/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.6110 - loss: 1.5969 - learning_rate: 0.0010\n",
            "Epoch 19/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.6240 - loss: 1.5344 - learning_rate: 0.0010\n",
            "Epoch 20/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6374 - loss: 1.4702 - learning_rate: 0.0010\n",
            "Epoch 21/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6462 - loss: 1.4084 - learning_rate: 0.0010\n",
            "Epoch 22/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6497 - loss: 1.3744 - learning_rate: 0.0010\n",
            "Epoch 23/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6641 - loss: 1.3137 - learning_rate: 0.0010\n",
            "Epoch 24/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6756 - loss: 1.2634 - learning_rate: 0.0010\n",
            "Epoch 25/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.6771 - loss: 1.2426 - learning_rate: 0.0010\n",
            "Epoch 26/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.6833 - loss: 1.2137 - learning_rate: 0.0010\n",
            "Epoch 27/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6887 - loss: 1.1834 - learning_rate: 0.0010\n",
            "Epoch 28/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6976 - loss: 1.1488 - learning_rate: 0.0010\n",
            "Epoch 29/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6977 - loss: 1.1542 - learning_rate: 0.0010\n",
            "Epoch 30/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7000 - loss: 1.1211 - learning_rate: 0.0010\n",
            "Epoch 31/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7083 - loss: 1.1007 - learning_rate: 0.0010\n",
            "Epoch 32/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7184 - loss: 1.0505 - learning_rate: 0.0010\n",
            "Epoch 33/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7185 - loss: 1.0388 - learning_rate: 0.0010\n",
            "Epoch 34/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7184 - loss: 1.0437 - learning_rate: 0.0010\n",
            "Epoch 35/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7232 - loss: 1.0193 - learning_rate: 0.0010\n",
            "Epoch 36/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7236 - loss: 1.0012 - learning_rate: 0.0010\n",
            "Epoch 37/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.7322 - loss: 0.9804 - learning_rate: 0.0010\n",
            "Epoch 38/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7305 - loss: 0.9843 - learning_rate: 0.0010\n",
            "Epoch 39/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7344 - loss: 0.9568 - learning_rate: 0.0010\n",
            "Epoch 40/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7340 - loss: 0.9624 - learning_rate: 0.0010\n",
            "Epoch 41/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7413 - loss: 0.9296 - learning_rate: 0.0010\n",
            "Epoch 42/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.7442 - loss: 0.9215 - learning_rate: 0.0010\n",
            "Epoch 43/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7460 - loss: 0.9191 - learning_rate: 0.0010\n",
            "Epoch 44/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7460 - loss: 0.9006 - learning_rate: 0.0010\n",
            "Epoch 45/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7484 - loss: 0.8935 - learning_rate: 0.0010\n",
            "Epoch 46/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7473 - loss: 0.9005 - learning_rate: 0.0010\n",
            "Epoch 47/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.7518 - loss: 0.8864 - learning_rate: 0.0010\n",
            "Epoch 48/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7595 - loss: 0.8630 - learning_rate: 0.0010\n",
            "Epoch 49/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7540 - loss: 0.8671 - learning_rate: 0.0010\n",
            "Epoch 50/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7556 - loss: 0.8635 - learning_rate: 0.0010\n",
            "Epoch 51/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7649 - loss: 0.8325 - learning_rate: 0.0010\n",
            "Epoch 52/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7678 - loss: 0.8328 - learning_rate: 0.0010\n",
            "Epoch 53/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7622 - loss: 0.8332 - learning_rate: 0.0010\n",
            "Epoch 54/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.7601 - loss: 0.8326 - learning_rate: 0.0010\n",
            "Epoch 55/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7712 - loss: 0.8124 - learning_rate: 0.0010\n",
            "Epoch 56/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7658 - loss: 0.8179 - learning_rate: 0.0010\n",
            "Epoch 57/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7730 - loss: 0.7958 - learning_rate: 0.0010\n",
            "Epoch 58/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7729 - loss: 0.7937 - learning_rate: 0.0010\n",
            "Epoch 59/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7731 - loss: 0.7984 - learning_rate: 0.0010\n",
            "Epoch 60/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.7810 - loss: 0.7699 - learning_rate: 0.0010\n",
            "Epoch 61/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7762 - loss: 0.7784 - learning_rate: 0.0010\n",
            "Epoch 62/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7779 - loss: 0.7754 - learning_rate: 0.0010\n",
            "Epoch 63/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7799 - loss: 0.7629 - learning_rate: 0.0010\n",
            "Epoch 64/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7843 - loss: 0.7564 - learning_rate: 0.0010\n",
            "Epoch 65/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.7832 - loss: 0.7563 - learning_rate: 0.0010\n",
            "Epoch 66/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7821 - loss: 0.7606 - learning_rate: 0.0010\n",
            "Epoch 67/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7844 - loss: 0.7538 - learning_rate: 0.0010\n",
            "Epoch 68/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7809 - loss: 0.7564 - learning_rate: 0.0010\n",
            "Epoch 69/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7906 - loss: 0.7239 - learning_rate: 0.0010\n",
            "Epoch 70/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.7826 - loss: 0.7468 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 1 LSTM layers and 128 units per layer...\n",
            "Epoch 1/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.0217 - loss: 7.5234 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.0453 - loss: 6.6881 - learning_rate: 0.0010\n",
            "Epoch 3/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.0883 - loss: 6.0201 - learning_rate: 0.0010\n",
            "Epoch 4/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.1265 - loss: 5.3492 - learning_rate: 0.0010\n",
            "Epoch 5/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1763 - loss: 4.6833 - learning_rate: 0.0010\n",
            "Epoch 6/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.2315 - loss: 4.0562 - learning_rate: 0.0010\n",
            "Epoch 7/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.3101 - loss: 3.4448 - learning_rate: 0.0010\n",
            "Epoch 8/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.3916 - loss: 2.9023 - learning_rate: 0.0010\n",
            "Epoch 9/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.4669 - loss: 2.4794 - learning_rate: 0.0010\n",
            "Epoch 10/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5092 - loss: 2.1951 - learning_rate: 0.0010\n",
            "Epoch 11/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5514 - loss: 1.9300 - learning_rate: 0.0010\n",
            "Epoch 12/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5855 - loss: 1.7539 - learning_rate: 0.0010\n",
            "Epoch 13/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6128 - loss: 1.5893 - learning_rate: 0.0010\n",
            "Epoch 14/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.6381 - loss: 1.4679 - learning_rate: 0.0010\n",
            "Epoch 15/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6569 - loss: 1.3645 - learning_rate: 0.0010\n",
            "Epoch 16/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6754 - loss: 1.2705 - learning_rate: 0.0010\n",
            "Epoch 17/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.6935 - loss: 1.1871 - learning_rate: 0.0010\n",
            "Epoch 18/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7033 - loss: 1.1227 - learning_rate: 0.0010\n",
            "Epoch 19/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7197 - loss: 1.0455 - learning_rate: 0.0010\n",
            "Epoch 20/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7274 - loss: 1.0180 - learning_rate: 0.0010\n",
            "Epoch 21/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7353 - loss: 0.9674 - learning_rate: 0.0010\n",
            "Epoch 22/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7507 - loss: 0.9186 - learning_rate: 0.0010\n",
            "Epoch 23/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7529 - loss: 0.8909 - learning_rate: 0.0010\n",
            "Epoch 24/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7655 - loss: 0.8472 - learning_rate: 0.0010\n",
            "Epoch 25/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7691 - loss: 0.8191 - learning_rate: 0.0010\n",
            "Epoch 26/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7815 - loss: 0.7817 - learning_rate: 0.0010\n",
            "Epoch 27/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7842 - loss: 0.7584 - learning_rate: 0.0010\n",
            "Epoch 28/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7875 - loss: 0.7426 - learning_rate: 0.0010\n",
            "Epoch 29/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7987 - loss: 0.7040 - learning_rate: 0.0010\n",
            "Epoch 30/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7961 - loss: 0.7093 - learning_rate: 0.0010\n",
            "Epoch 31/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8077 - loss: 0.6755 - learning_rate: 0.0010\n",
            "Epoch 32/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8077 - loss: 0.6660 - learning_rate: 0.0010\n",
            "Epoch 33/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8128 - loss: 0.6454 - learning_rate: 0.0010\n",
            "Epoch 34/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8202 - loss: 0.6243 - learning_rate: 0.0010\n",
            "Epoch 35/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8204 - loss: 0.6185 - learning_rate: 0.0010\n",
            "Epoch 36/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8279 - loss: 0.5849 - learning_rate: 0.0010\n",
            "Epoch 37/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8270 - loss: 0.5843 - learning_rate: 0.0010\n",
            "Epoch 38/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8325 - loss: 0.5728 - learning_rate: 0.0010\n",
            "Epoch 39/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8347 - loss: 0.5578 - learning_rate: 0.0010\n",
            "Epoch 40/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8384 - loss: 0.5467 - learning_rate: 0.0010\n",
            "Epoch 41/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8355 - loss: 0.5566 - learning_rate: 0.0010\n",
            "Epoch 42/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8406 - loss: 0.5330 - learning_rate: 0.0010\n",
            "Epoch 43/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8495 - loss: 0.5120 - learning_rate: 0.0010\n",
            "Epoch 44/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8509 - loss: 0.5052 - learning_rate: 0.0010\n",
            "Epoch 45/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8467 - loss: 0.5084 - learning_rate: 0.0010\n",
            "Epoch 46/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8449 - loss: 0.5117 - learning_rate: 0.0010\n",
            "Epoch 47/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.8542 - loss: 0.4918 - learning_rate: 0.0010\n",
            "Epoch 48/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.8567 - loss: 0.4839 - learning_rate: 0.0010\n",
            "Epoch 49/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8587 - loss: 0.4712 - learning_rate: 0.0010\n",
            "Epoch 50/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8624 - loss: 0.4592 - learning_rate: 0.0010\n",
            "Epoch 51/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8572 - loss: 0.4663 - learning_rate: 0.0010\n",
            "Epoch 52/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8614 - loss: 0.4716 - learning_rate: 0.0010\n",
            "Epoch 53/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8669 - loss: 0.4577 - learning_rate: 0.0010\n",
            "Epoch 54/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8685 - loss: 0.4421 - learning_rate: 0.0010\n",
            "Epoch 55/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8694 - loss: 0.4294 - learning_rate: 0.0010\n",
            "Epoch 56/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8669 - loss: 0.4389 - learning_rate: 0.0010\n",
            "Epoch 57/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8701 - loss: 0.4246 - learning_rate: 0.0010\n",
            "Epoch 58/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8678 - loss: 0.4353 - learning_rate: 0.0010\n",
            "Epoch 59/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8702 - loss: 0.4274 - learning_rate: 0.0010\n",
            "Epoch 60/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8758 - loss: 0.4143 - learning_rate: 0.0010\n",
            "Epoch 61/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8764 - loss: 0.4134 - learning_rate: 0.0010\n",
            "Epoch 62/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8757 - loss: 0.4088 - learning_rate: 0.0010\n",
            "Epoch 63/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8794 - loss: 0.3989 - learning_rate: 0.0010\n",
            "Epoch 64/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8773 - loss: 0.3957 - learning_rate: 0.0010\n",
            "Epoch 65/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8829 - loss: 0.3859 - learning_rate: 0.0010\n",
            "Epoch 66/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8785 - loss: 0.3995 - learning_rate: 0.0010\n",
            "Epoch 67/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8880 - loss: 0.3719 - learning_rate: 0.0010\n",
            "Epoch 68/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8832 - loss: 0.3827 - learning_rate: 0.0010\n",
            "Epoch 69/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8796 - loss: 0.3934 - learning_rate: 0.0010\n",
            "Epoch 70/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8890 - loss: 0.3648 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 1 LSTM layers and 256 units per layer...\n",
            "Epoch 1/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.0186 - loss: 7.9828 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.0243 - loss: 7.0338 - learning_rate: 0.0010\n",
            "Epoch 3/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.0290 - loss: 6.8358 - learning_rate: 0.0010\n",
            "Epoch 4/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0503 - loss: 6.5167 - learning_rate: 0.0010\n",
            "Epoch 5/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.0720 - loss: 6.1593 - learning_rate: 0.0010\n",
            "Epoch 6/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0969 - loss: 5.7255 - learning_rate: 0.0010\n",
            "Epoch 7/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1299 - loss: 5.1926 - learning_rate: 0.0010\n",
            "Epoch 8/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.1661 - loss: 4.6306 - learning_rate: 0.0010\n",
            "Epoch 9/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.2259 - loss: 4.0438 - learning_rate: 0.0010\n",
            "Epoch 10/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.2995 - loss: 3.4956 - learning_rate: 0.0010\n",
            "Epoch 11/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3803 - loss: 2.9702 - learning_rate: 0.0010\n",
            "Epoch 12/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.4428 - loss: 2.5806 - learning_rate: 0.0010\n",
            "Epoch 13/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4983 - loss: 2.2387 - learning_rate: 0.0010\n",
            "Epoch 14/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5404 - loss: 1.9902 - learning_rate: 0.0010\n",
            "Epoch 15/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.5959 - loss: 1.7292 - learning_rate: 0.0010\n",
            "Epoch 16/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.6265 - loss: 1.5504 - learning_rate: 0.0010\n",
            "Epoch 17/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.6596 - loss: 1.3797 - learning_rate: 0.0010\n",
            "Epoch 18/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6873 - loss: 1.2347 - learning_rate: 0.0010\n",
            "Epoch 19/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.7127 - loss: 1.1211 - learning_rate: 0.0010\n",
            "Epoch 20/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.7349 - loss: 1.0084 - learning_rate: 0.0010\n",
            "Epoch 21/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.7581 - loss: 0.9000 - learning_rate: 0.0010\n",
            "Epoch 22/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.7725 - loss: 0.8281 - learning_rate: 0.0010\n",
            "Epoch 23/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.7890 - loss: 0.7631 - learning_rate: 0.0010\n",
            "Epoch 24/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.8124 - loss: 0.6719 - learning_rate: 0.0010\n",
            "Epoch 25/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8261 - loss: 0.6228 - learning_rate: 0.0010\n",
            "Epoch 26/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8358 - loss: 0.5684 - learning_rate: 0.0010\n",
            "Epoch 27/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.8469 - loss: 0.5283 - learning_rate: 0.0010\n",
            "Epoch 28/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8560 - loss: 0.4924 - learning_rate: 0.0010\n",
            "Epoch 29/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.8636 - loss: 0.4600 - learning_rate: 0.0010\n",
            "Epoch 30/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.8731 - loss: 0.4324 - learning_rate: 0.0010\n",
            "Epoch 31/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.8833 - loss: 0.3962 - learning_rate: 0.0010\n",
            "Epoch 32/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8847 - loss: 0.3855 - learning_rate: 0.0010\n",
            "Epoch 33/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.8918 - loss: 0.3680 - learning_rate: 0.0010\n",
            "Epoch 34/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9001 - loss: 0.3358 - learning_rate: 0.0010\n",
            "Epoch 35/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.8968 - loss: 0.3363 - learning_rate: 0.0010\n",
            "Epoch 36/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9050 - loss: 0.3120 - learning_rate: 0.0010\n",
            "Epoch 37/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9117 - loss: 0.2913 - learning_rate: 0.0010\n",
            "Epoch 38/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9153 - loss: 0.2792 - learning_rate: 0.0010\n",
            "Epoch 39/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9092 - loss: 0.3027 - learning_rate: 0.0010\n",
            "Epoch 40/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9187 - loss: 0.2696 - learning_rate: 0.0010\n",
            "Epoch 41/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9234 - loss: 0.2490 - learning_rate: 0.0010\n",
            "Epoch 42/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9176 - loss: 0.2605 - learning_rate: 0.0010\n",
            "Epoch 43/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9270 - loss: 0.2402 - learning_rate: 0.0010\n",
            "Epoch 44/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9239 - loss: 0.2464 - learning_rate: 0.0010\n",
            "Epoch 45/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9311 - loss: 0.2272 - learning_rate: 0.0010\n",
            "Epoch 46/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9300 - loss: 0.2266 - learning_rate: 0.0010\n",
            "Epoch 47/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9332 - loss: 0.2146 - learning_rate: 0.0010\n",
            "Epoch 48/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9353 - loss: 0.2083 - learning_rate: 0.0010\n",
            "Epoch 49/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9361 - loss: 0.2090 - learning_rate: 0.0010\n",
            "Epoch 50/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9388 - loss: 0.2022 - learning_rate: 0.0010\n",
            "Epoch 51/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9405 - loss: 0.1925 - learning_rate: 0.0010\n",
            "Epoch 52/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9428 - loss: 0.1837 - learning_rate: 0.0010\n",
            "Epoch 53/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9386 - loss: 0.1975 - learning_rate: 0.0010\n",
            "Epoch 54/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9353 - loss: 0.2145 - learning_rate: 0.0010\n",
            "Epoch 55/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9439 - loss: 0.1814 - learning_rate: 0.0010\n",
            "Epoch 56/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9444 - loss: 0.1772 - learning_rate: 0.0010\n",
            "Epoch 57/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9473 - loss: 0.1679 - learning_rate: 0.0010\n",
            "Epoch 58/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9436 - loss: 0.1841 - learning_rate: 0.0010\n",
            "Epoch 59/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9471 - loss: 0.1706 - learning_rate: 0.0010\n",
            "Epoch 60/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9439 - loss: 0.1765 - learning_rate: 0.0010\n",
            "Epoch 61/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9499 - loss: 0.1627 - learning_rate: 0.0010\n",
            "Epoch 62/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9489 - loss: 0.1667 - learning_rate: 0.0010\n",
            "Epoch 63/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9436 - loss: 0.1808 - learning_rate: 0.0010\n",
            "Epoch 64/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9500 - loss: 0.1610 - learning_rate: 0.0010\n",
            "Epoch 65/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9489 - loss: 0.1651 - learning_rate: 0.0010\n",
            "Epoch 66/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9536 - loss: 0.1452 - learning_rate: 0.0010\n",
            "Epoch 67/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9536 - loss: 0.1480 - learning_rate: 0.0010\n",
            "Epoch 68/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9515 - loss: 0.1596 - learning_rate: 0.0010\n",
            "Epoch 69/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9528 - loss: 0.1491 - learning_rate: 0.0010\n",
            "Epoch 70/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9516 - loss: 0.1566 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 2 LSTM layers and 64 units per layer...\n",
            "Epoch 1/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.0220 - loss: 7.5954 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.0278 - loss: 6.8951 - learning_rate: 0.0010\n",
            "Epoch 3/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.0387 - loss: 6.6476 - learning_rate: 0.0010\n",
            "Epoch 4/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.0572 - loss: 6.3238 - learning_rate: 0.0010\n",
            "Epoch 5/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.0720 - loss: 6.0492 - learning_rate: 0.0010\n",
            "Epoch 6/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.0821 - loss: 5.7443 - learning_rate: 0.0010\n",
            "Epoch 7/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.0940 - loss: 5.4858 - learning_rate: 0.0010\n",
            "Epoch 8/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.1083 - loss: 5.2220 - learning_rate: 0.0010\n",
            "Epoch 9/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.1155 - loss: 4.9790 - learning_rate: 0.0010\n",
            "Epoch 10/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.1339 - loss: 4.7389 - learning_rate: 0.0010\n",
            "Epoch 11/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.1496 - loss: 4.5436 - learning_rate: 0.0010\n",
            "Epoch 12/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.1727 - loss: 4.3225 - learning_rate: 0.0010\n",
            "Epoch 13/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.1969 - loss: 4.1373 - learning_rate: 0.0010\n",
            "Epoch 14/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.2224 - loss: 3.9440 - learning_rate: 0.0010\n",
            "Epoch 15/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.2479 - loss: 3.7859 - learning_rate: 0.0010\n",
            "Epoch 16/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.2702 - loss: 3.6356 - learning_rate: 0.0010\n",
            "Epoch 17/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.2902 - loss: 3.4923 - learning_rate: 0.0010\n",
            "Epoch 18/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3018 - loss: 3.3723 - learning_rate: 0.0010\n",
            "Epoch 19/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.3120 - loss: 3.2904 - learning_rate: 0.0010\n",
            "Epoch 20/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3223 - loss: 3.2077 - learning_rate: 0.0010\n",
            "Epoch 21/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.3376 - loss: 3.1152 - learning_rate: 0.0010\n",
            "Epoch 22/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3495 - loss: 3.0441 - learning_rate: 0.0010\n",
            "Epoch 23/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3594 - loss: 2.9575 - learning_rate: 0.0010\n",
            "Epoch 24/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.3695 - loss: 2.8967 - learning_rate: 0.0010\n",
            "Epoch 25/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.3760 - loss: 2.8442 - learning_rate: 0.0010\n",
            "Epoch 26/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3830 - loss: 2.7968 - learning_rate: 0.0010\n",
            "Epoch 27/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.3900 - loss: 2.7369 - learning_rate: 0.0010\n",
            "Epoch 28/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4006 - loss: 2.6912 - learning_rate: 0.0010\n",
            "Epoch 29/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.4041 - loss: 2.6611 - learning_rate: 0.0010\n",
            "Epoch 30/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.4104 - loss: 2.6068 - learning_rate: 0.0010\n",
            "Epoch 31/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4189 - loss: 2.5762 - learning_rate: 0.0010\n",
            "Epoch 32/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4181 - loss: 2.5380 - learning_rate: 0.0010\n",
            "Epoch 33/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.4256 - loss: 2.5173 - learning_rate: 0.0010\n",
            "Epoch 34/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4327 - loss: 2.4733 - learning_rate: 0.0010\n",
            "Epoch 35/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4308 - loss: 2.4605 - learning_rate: 0.0010\n",
            "Epoch 36/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.4390 - loss: 2.4126 - learning_rate: 0.0010\n",
            "Epoch 37/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4464 - loss: 2.3848 - learning_rate: 0.0010\n",
            "Epoch 38/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4443 - loss: 2.3881 - learning_rate: 0.0010\n",
            "Epoch 39/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4508 - loss: 2.3390 - learning_rate: 0.0010\n",
            "Epoch 40/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4595 - loss: 2.3109 - learning_rate: 0.0010\n",
            "Epoch 41/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4627 - loss: 2.2870 - learning_rate: 0.0010\n",
            "Epoch 42/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4631 - loss: 2.2833 - learning_rate: 0.0010\n",
            "Epoch 43/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.4652 - loss: 2.2622 - learning_rate: 0.0010\n",
            "Epoch 44/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4673 - loss: 2.2346 - learning_rate: 0.0010\n",
            "Epoch 45/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4767 - loss: 2.1999 - learning_rate: 0.0010\n",
            "Epoch 46/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4704 - loss: 2.2104 - learning_rate: 0.0010\n",
            "Epoch 47/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4802 - loss: 2.1625 - learning_rate: 0.0010\n",
            "Epoch 48/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4813 - loss: 2.1497 - learning_rate: 0.0010\n",
            "Epoch 49/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.4848 - loss: 2.1291 - learning_rate: 0.0010\n",
            "Epoch 50/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.4895 - loss: 2.1098 - learning_rate: 0.0010\n",
            "Epoch 51/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.4917 - loss: 2.1013 - learning_rate: 0.0010\n",
            "Epoch 52/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4918 - loss: 2.0842 - learning_rate: 0.0010\n",
            "Epoch 53/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.4940 - loss: 2.0894 - learning_rate: 0.0010\n",
            "Epoch 54/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5016 - loss: 2.0651 - learning_rate: 0.0010\n",
            "Epoch 55/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.5047 - loss: 2.0522 - learning_rate: 0.0010\n",
            "Epoch 56/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.5020 - loss: 2.0431 - learning_rate: 0.0010\n",
            "Epoch 57/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.5036 - loss: 2.0138 - learning_rate: 0.0010\n",
            "Epoch 58/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.5060 - loss: 2.0088 - learning_rate: 0.0010\n",
            "Epoch 59/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.5079 - loss: 2.0015 - learning_rate: 0.0010\n",
            "Epoch 60/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.5078 - loss: 1.9921 - learning_rate: 0.0010\n",
            "Epoch 61/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.5156 - loss: 1.9655 - learning_rate: 0.0010\n",
            "Epoch 62/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.5122 - loss: 1.9769 - learning_rate: 0.0010\n",
            "Epoch 63/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5226 - loss: 1.9384 - learning_rate: 0.0010\n",
            "Epoch 64/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.5214 - loss: 1.9319 - learning_rate: 0.0010\n",
            "Epoch 65/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.5278 - loss: 1.9134 - learning_rate: 0.0010\n",
            "Epoch 66/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.5170 - loss: 1.9463 - learning_rate: 0.0010\n",
            "Epoch 67/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.5266 - loss: 1.9194 - learning_rate: 0.0010\n",
            "Epoch 68/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.5282 - loss: 1.8995 - learning_rate: 0.0010\n",
            "Epoch 69/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.5341 - loss: 1.8875 - learning_rate: 0.0010\n",
            "Epoch 70/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.5342 - loss: 1.8761 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 2 LSTM layers and 128 units per layer...\n",
            "Epoch 1/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0216 - loss: 7.5046 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0248 - loss: 6.9712 - learning_rate: 0.0010\n",
            "Epoch 3/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0361 - loss: 6.7294 - learning_rate: 0.0010\n",
            "Epoch 4/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.0476 - loss: 6.5298 - learning_rate: 0.0010\n",
            "Epoch 5/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0571 - loss: 6.3171 - learning_rate: 0.0010\n",
            "Epoch 6/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0679 - loss: 6.1112 - learning_rate: 0.0010\n",
            "Epoch 7/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.0734 - loss: 5.8979 - learning_rate: 0.0010\n",
            "Epoch 8/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.0897 - loss: 5.6323 - learning_rate: 0.0010\n",
            "Epoch 9/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0939 - loss: 5.3919 - learning_rate: 0.0010\n",
            "Epoch 10/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.1028 - loss: 5.1595 - learning_rate: 0.0010\n",
            "Epoch 11/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.1128 - loss: 4.9202 - learning_rate: 0.0010\n",
            "Epoch 12/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1297 - loss: 4.6784 - learning_rate: 0.0010\n",
            "Epoch 13/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.1470 - loss: 4.4569 - learning_rate: 0.0010\n",
            "Epoch 14/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.1676 - loss: 4.2478 - learning_rate: 0.0010\n",
            "Epoch 15/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1968 - loss: 4.0533 - learning_rate: 0.0010\n",
            "Epoch 16/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.2228 - loss: 3.8524 - learning_rate: 0.0010\n",
            "Epoch 17/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.2531 - loss: 3.6700 - learning_rate: 0.0010\n",
            "Epoch 18/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.2657 - loss: 3.5222 - learning_rate: 0.0010\n",
            "Epoch 19/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.2915 - loss: 3.3802 - learning_rate: 0.0010\n",
            "Epoch 20/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.3202 - loss: 3.2178 - learning_rate: 0.0010\n",
            "Epoch 21/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3290 - loss: 3.1161 - learning_rate: 0.0010\n",
            "Epoch 22/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3477 - loss: 2.9992 - learning_rate: 0.0010\n",
            "Epoch 23/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3655 - loss: 2.8962 - learning_rate: 0.0010\n",
            "Epoch 24/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3791 - loss: 2.8104 - learning_rate: 0.0010\n",
            "Epoch 25/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3926 - loss: 2.7305 - learning_rate: 0.0010\n",
            "Epoch 26/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4031 - loss: 2.6542 - learning_rate: 0.0010\n",
            "Epoch 27/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4065 - loss: 2.6094 - learning_rate: 0.0010\n",
            "Epoch 28/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4227 - loss: 2.5268 - learning_rate: 0.0010\n",
            "Epoch 29/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4346 - loss: 2.4667 - learning_rate: 0.0010\n",
            "Epoch 30/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.4387 - loss: 2.4300 - learning_rate: 0.0010\n",
            "Epoch 31/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4471 - loss: 2.3627 - learning_rate: 0.0010\n",
            "Epoch 32/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.4590 - loss: 2.3061 - learning_rate: 0.0010\n",
            "Epoch 33/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.4690 - loss: 2.2567 - learning_rate: 0.0010\n",
            "Epoch 34/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4714 - loss: 2.2308 - learning_rate: 0.0010\n",
            "Epoch 35/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4817 - loss: 2.1734 - learning_rate: 0.0010\n",
            "Epoch 36/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4917 - loss: 2.1446 - learning_rate: 0.0010\n",
            "Epoch 37/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.4982 - loss: 2.0989 - learning_rate: 0.0010\n",
            "Epoch 38/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5030 - loss: 2.0612 - learning_rate: 0.0010\n",
            "Epoch 39/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5091 - loss: 2.0306 - learning_rate: 0.0010\n",
            "Epoch 40/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.5117 - loss: 1.9993 - learning_rate: 0.0010\n",
            "Epoch 41/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.5192 - loss: 1.9588 - learning_rate: 0.0010\n",
            "Epoch 42/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.5274 - loss: 1.9335 - learning_rate: 0.0010\n",
            "Epoch 43/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5361 - loss: 1.9006 - learning_rate: 0.0010\n",
            "Epoch 44/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5417 - loss: 1.8511 - learning_rate: 0.0010\n",
            "Epoch 45/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.5410 - loss: 1.8488 - learning_rate: 0.0010\n",
            "Epoch 46/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5429 - loss: 1.8432 - learning_rate: 0.0010\n",
            "Epoch 47/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.5541 - loss: 1.8009 - learning_rate: 0.0010\n",
            "Epoch 48/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5545 - loss: 1.7878 - learning_rate: 0.0010\n",
            "Epoch 49/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.5591 - loss: 1.7475 - learning_rate: 0.0010\n",
            "Epoch 50/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5673 - loss: 1.7221 - learning_rate: 0.0010\n",
            "Epoch 51/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5740 - loss: 1.6824 - learning_rate: 0.0010\n",
            "Epoch 52/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5760 - loss: 1.6715 - learning_rate: 0.0010\n",
            "Epoch 53/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.5734 - loss: 1.6646 - learning_rate: 0.0010\n",
            "Epoch 54/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5853 - loss: 1.6257 - learning_rate: 0.0010\n",
            "Epoch 55/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5854 - loss: 1.6280 - learning_rate: 0.0010\n",
            "Epoch 56/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5910 - loss: 1.5967 - learning_rate: 0.0010\n",
            "Epoch 57/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.5979 - loss: 1.5612 - learning_rate: 0.0010\n",
            "Epoch 58/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.5974 - loss: 1.5602 - learning_rate: 0.0010\n",
            "Epoch 59/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.5994 - loss: 1.5379 - learning_rate: 0.0010\n",
            "Epoch 60/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6031 - loss: 1.5339 - learning_rate: 0.0010\n",
            "Epoch 61/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.6097 - loss: 1.5020 - learning_rate: 0.0010\n",
            "Epoch 62/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6099 - loss: 1.5005 - learning_rate: 0.0010\n",
            "Epoch 63/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.6174 - loss: 1.4650 - learning_rate: 0.0010\n",
            "Epoch 64/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.6194 - loss: 1.4564 - learning_rate: 0.0010\n",
            "Epoch 65/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.6266 - loss: 1.4363 - learning_rate: 0.0010\n",
            "Epoch 66/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6224 - loss: 1.4400 - learning_rate: 0.0010\n",
            "Epoch 67/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6283 - loss: 1.4078 - learning_rate: 0.0010\n",
            "Epoch 68/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.6280 - loss: 1.4028 - learning_rate: 0.0010\n",
            "Epoch 69/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6281 - loss: 1.4044 - learning_rate: 0.0010\n",
            "Epoch 70/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6406 - loss: 1.3632 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 2 LSTM layers and 256 units per layer...\n",
            "Epoch 1/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 38ms/step - accuracy: 0.0232 - loss: 7.5007 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.0231 - loss: 7.0949 - learning_rate: 0.0010\n",
            "Epoch 3/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.0245 - loss: 7.0220 - learning_rate: 0.0010\n",
            "Epoch 4/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.0249 - loss: 6.9114 - learning_rate: 0.0010\n",
            "Epoch 5/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.0357 - loss: 6.7097 - learning_rate: 0.0010\n",
            "Epoch 6/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.0465 - loss: 6.5184 - learning_rate: 0.0010\n",
            "Epoch 7/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.0571 - loss: 6.3171 - learning_rate: 0.0010\n",
            "Epoch 8/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.0628 - loss: 6.1286 - learning_rate: 0.0010\n",
            "Epoch 9/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.0742 - loss: 5.9233 - learning_rate: 0.0010\n",
            "Epoch 10/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.0851 - loss: 5.6910 - learning_rate: 0.0010\n",
            "Epoch 11/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.0967 - loss: 5.4433 - learning_rate: 0.0010\n",
            "Epoch 12/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.1080 - loss: 5.1981 - learning_rate: 0.0010\n",
            "Epoch 13/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.1200 - loss: 4.9458 - learning_rate: 0.0010\n",
            "Epoch 14/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.1270 - loss: 4.7150 - learning_rate: 0.0010\n",
            "Epoch 15/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.1469 - loss: 4.4731 - learning_rate: 0.0010\n",
            "Epoch 16/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.1726 - loss: 4.1979 - learning_rate: 0.0010\n",
            "Epoch 17/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.2040 - loss: 3.9370 - learning_rate: 0.0010\n",
            "Epoch 18/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.2378 - loss: 3.6884 - learning_rate: 0.0010\n",
            "Epoch 19/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.2690 - loss: 3.4567 - learning_rate: 0.0010\n",
            "Epoch 20/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.2997 - loss: 3.2539 - learning_rate: 0.0010\n",
            "Epoch 21/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.3344 - loss: 3.0529 - learning_rate: 0.0010\n",
            "Epoch 22/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.3672 - loss: 2.8636 - learning_rate: 0.0010\n",
            "Epoch 23/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.3950 - loss: 2.6839 - learning_rate: 0.0010\n",
            "Epoch 24/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.4197 - loss: 2.5419 - learning_rate: 0.0010\n",
            "Epoch 25/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.4465 - loss: 2.4021 - learning_rate: 0.0010\n",
            "Epoch 26/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.4675 - loss: 2.2903 - learning_rate: 0.0010\n",
            "Epoch 27/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.4824 - loss: 2.1844 - learning_rate: 0.0010\n",
            "Epoch 28/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.5064 - loss: 2.0675 - learning_rate: 0.0010\n",
            "Epoch 29/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.5194 - loss: 1.9805 - learning_rate: 0.0010\n",
            "Epoch 30/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.5334 - loss: 1.9140 - learning_rate: 0.0010\n",
            "Epoch 31/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.5495 - loss: 1.8251 - learning_rate: 0.0010\n",
            "Epoch 32/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.5671 - loss: 1.7333 - learning_rate: 0.0010\n",
            "Epoch 33/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.5877 - loss: 1.6465 - learning_rate: 0.0010\n",
            "Epoch 34/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.5990 - loss: 1.5933 - learning_rate: 0.0010\n",
            "Epoch 35/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.6116 - loss: 1.5348 - learning_rate: 0.0010\n",
            "Epoch 36/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.6238 - loss: 1.4669 - learning_rate: 0.0010\n",
            "Epoch 37/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.6330 - loss: 1.4118 - learning_rate: 0.0010\n",
            "Epoch 38/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.6437 - loss: 1.3620 - learning_rate: 0.0010\n",
            "Epoch 39/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.6680 - loss: 1.2879 - learning_rate: 0.0010\n",
            "Epoch 40/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.6691 - loss: 1.2530 - learning_rate: 0.0010\n",
            "Epoch 41/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.6840 - loss: 1.1910 - learning_rate: 0.0010\n",
            "Epoch 42/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.6936 - loss: 1.1405 - learning_rate: 0.0010\n",
            "Epoch 43/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.6998 - loss: 1.1208 - learning_rate: 0.0010\n",
            "Epoch 44/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.7120 - loss: 1.0638 - learning_rate: 0.0010\n",
            "Epoch 45/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.7210 - loss: 1.0288 - learning_rate: 0.0010\n",
            "Epoch 46/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.7280 - loss: 0.9994 - learning_rate: 0.0010\n",
            "Epoch 47/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.7403 - loss: 0.9484 - learning_rate: 0.0010\n",
            "Epoch 48/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.7486 - loss: 0.9089 - learning_rate: 0.0010\n",
            "Epoch 49/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.7529 - loss: 0.8865 - learning_rate: 0.0010\n",
            "Epoch 50/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.7627 - loss: 0.8483 - learning_rate: 0.0010\n",
            "Epoch 51/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.7739 - loss: 0.8123 - learning_rate: 0.0010\n",
            "Epoch 52/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.7781 - loss: 0.7855 - learning_rate: 0.0010\n",
            "Epoch 53/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.7857 - loss: 0.7510 - learning_rate: 0.0010\n",
            "Epoch 54/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.7954 - loss: 0.7250 - learning_rate: 0.0010\n",
            "Epoch 55/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.7959 - loss: 0.7170 - learning_rate: 0.0010\n",
            "Epoch 56/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.8040 - loss: 0.6841 - learning_rate: 0.0010\n",
            "Epoch 57/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.8122 - loss: 0.6527 - learning_rate: 0.0010\n",
            "Epoch 58/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.8187 - loss: 0.6335 - learning_rate: 0.0010\n",
            "Epoch 59/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.8159 - loss: 0.6325 - learning_rate: 0.0010\n",
            "Epoch 60/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.8276 - loss: 0.5850 - learning_rate: 0.0010\n",
            "Epoch 61/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.8329 - loss: 0.5696 - learning_rate: 0.0010\n",
            "Epoch 62/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.8359 - loss: 0.5515 - learning_rate: 0.0010\n",
            "Epoch 63/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.8444 - loss: 0.5290 - learning_rate: 0.0010\n",
            "Epoch 64/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.8484 - loss: 0.5120 - learning_rate: 0.0010\n",
            "Epoch 65/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.8495 - loss: 0.5038 - learning_rate: 0.0010\n",
            "Epoch 66/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.8597 - loss: 0.4754 - learning_rate: 0.0010\n",
            "Epoch 67/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.8607 - loss: 0.4681 - learning_rate: 0.0010\n",
            "Epoch 68/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.8580 - loss: 0.4684 - learning_rate: 0.0010\n",
            "Epoch 69/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.8678 - loss: 0.4465 - learning_rate: 0.0010\n",
            "Epoch 70/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.8738 - loss: 0.4220 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 3 LSTM layers and 64 units per layer...\n",
            "Epoch 1/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.0235 - loss: 7.5914 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.0234 - loss: 6.9783 - learning_rate: 0.0010\n",
            "Epoch 3/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.0275 - loss: 6.9099 - learning_rate: 0.0010\n",
            "Epoch 4/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.0371 - loss: 6.7345 - learning_rate: 0.0010\n",
            "Epoch 5/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0500 - loss: 6.5023 - learning_rate: 0.0010\n",
            "Epoch 6/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.0526 - loss: 6.3319 - learning_rate: 0.0010\n",
            "Epoch 7/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0612 - loss: 6.1385 - learning_rate: 0.0010\n",
            "Epoch 8/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0651 - loss: 5.9705 - learning_rate: 0.0010\n",
            "Epoch 9/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.0684 - loss: 5.8452 - learning_rate: 0.0010\n",
            "Epoch 10/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.0744 - loss: 5.6922 - learning_rate: 0.0010\n",
            "Epoch 11/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0769 - loss: 5.5368 - learning_rate: 0.0010\n",
            "Epoch 12/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.0819 - loss: 5.4057 - learning_rate: 0.0010\n",
            "Epoch 13/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.0889 - loss: 5.2638 - learning_rate: 0.0010\n",
            "Epoch 14/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.0929 - loss: 5.1262 - learning_rate: 0.0010\n",
            "Epoch 15/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.0978 - loss: 5.0024 - learning_rate: 0.0010\n",
            "Epoch 16/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.1042 - loss: 4.8627 - learning_rate: 0.0010\n",
            "Epoch 17/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.1171 - loss: 4.7367 - learning_rate: 0.0010\n",
            "Epoch 18/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.1268 - loss: 4.6266 - learning_rate: 0.0010\n",
            "Epoch 19/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.1348 - loss: 4.5080 - learning_rate: 0.0010\n",
            "Epoch 20/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.1498 - loss: 4.3990 - learning_rate: 0.0010\n",
            "Epoch 21/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.1653 - loss: 4.2931 - learning_rate: 0.0010\n",
            "Epoch 22/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.1770 - loss: 4.1971 - learning_rate: 0.0010\n",
            "Epoch 23/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.1939 - loss: 4.0934 - learning_rate: 0.0010\n",
            "Epoch 24/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.2114 - loss: 3.9905 - learning_rate: 0.0010\n",
            "Epoch 25/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.2192 - loss: 3.9147 - learning_rate: 0.0010\n",
            "Epoch 26/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.2310 - loss: 3.8243 - learning_rate: 0.0010\n",
            "Epoch 27/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.2397 - loss: 3.7697 - learning_rate: 0.0010\n",
            "Epoch 28/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.2497 - loss: 3.6804 - learning_rate: 0.0010\n",
            "Epoch 29/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.2613 - loss: 3.6259 - learning_rate: 0.0010\n",
            "Epoch 30/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.2716 - loss: 3.5526 - learning_rate: 0.0010\n",
            "Epoch 31/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.2790 - loss: 3.4936 - learning_rate: 0.0010\n",
            "Epoch 32/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.2868 - loss: 3.4290 - learning_rate: 0.0010\n",
            "Epoch 33/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.2930 - loss: 3.3790 - learning_rate: 0.0010\n",
            "Epoch 34/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.2991 - loss: 3.3367 - learning_rate: 0.0010\n",
            "Epoch 35/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3062 - loss: 3.2946 - learning_rate: 0.0010\n",
            "Epoch 36/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.3130 - loss: 3.2777 - learning_rate: 0.0010\n",
            "Epoch 37/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3158 - loss: 3.2273 - learning_rate: 0.0010\n",
            "Epoch 38/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.3219 - loss: 3.2005 - learning_rate: 0.0010\n",
            "Epoch 39/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.3250 - loss: 3.1681 - learning_rate: 0.0010\n",
            "Epoch 40/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.3293 - loss: 3.1329 - learning_rate: 0.0010\n",
            "Epoch 41/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3422 - loss: 3.0779 - learning_rate: 0.0010\n",
            "Epoch 42/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.3412 - loss: 3.0500 - learning_rate: 0.0010\n",
            "Epoch 43/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.3470 - loss: 3.0216 - learning_rate: 0.0010\n",
            "Epoch 44/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.3473 - loss: 2.9998 - learning_rate: 0.0010\n",
            "Epoch 45/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.3520 - loss: 2.9730 - learning_rate: 0.0010\n",
            "Epoch 46/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3541 - loss: 2.9417 - learning_rate: 0.0010\n",
            "Epoch 47/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3655 - loss: 2.9116 - learning_rate: 0.0010\n",
            "Epoch 48/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3668 - loss: 2.8880 - learning_rate: 0.0010\n",
            "Epoch 49/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.3679 - loss: 2.8638 - learning_rate: 0.0010\n",
            "Epoch 50/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.3714 - loss: 2.8506 - learning_rate: 0.0010\n",
            "Epoch 51/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3732 - loss: 2.8436 - learning_rate: 0.0010\n",
            "Epoch 52/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.3719 - loss: 2.8295 - learning_rate: 0.0010\n",
            "Epoch 53/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.3769 - loss: 2.7980 - learning_rate: 0.0010\n",
            "Epoch 54/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3809 - loss: 2.7582 - learning_rate: 0.0010\n",
            "Epoch 55/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3871 - loss: 2.7569 - learning_rate: 0.0010\n",
            "Epoch 56/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3875 - loss: 2.7336 - learning_rate: 0.0010\n",
            "Epoch 57/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.3949 - loss: 2.7028 - learning_rate: 0.0010\n",
            "Epoch 58/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3906 - loss: 2.7095 - learning_rate: 0.0010\n",
            "Epoch 59/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.3957 - loss: 2.6832 - learning_rate: 0.0010\n",
            "Epoch 60/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3965 - loss: 2.6755 - learning_rate: 0.0010\n",
            "Epoch 61/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.4044 - loss: 2.6511 - learning_rate: 0.0010\n",
            "Epoch 62/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.4037 - loss: 2.6396 - learning_rate: 0.0010\n",
            "Epoch 63/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.4046 - loss: 2.6397 - learning_rate: 0.0010\n",
            "Epoch 64/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4091 - loss: 2.6050 - learning_rate: 0.0010\n",
            "Epoch 65/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.4046 - loss: 2.6079 - learning_rate: 0.0010\n",
            "Epoch 66/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.4131 - loss: 2.5837 - learning_rate: 0.0010\n",
            "Epoch 67/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.4145 - loss: 2.5734 - learning_rate: 0.0010\n",
            "Epoch 68/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.4174 - loss: 2.5493 - learning_rate: 0.0010\n",
            "Epoch 69/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.4151 - loss: 2.5459 - learning_rate: 0.0010\n",
            "Epoch 70/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.4194 - loss: 2.5337 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 3 LSTM layers and 128 units per layer...\n",
            "Epoch 1/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 28ms/step - accuracy: 0.0219 - loss: 7.5107 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.0244 - loss: 7.0226 - learning_rate: 0.0010\n",
            "Epoch 3/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.0255 - loss: 6.9558 - learning_rate: 0.0010\n",
            "Epoch 4/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0270 - loss: 6.8697 - learning_rate: 0.0010\n",
            "Epoch 5/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.0376 - loss: 6.6333 - learning_rate: 0.0010\n",
            "Epoch 6/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0456 - loss: 6.4689 - learning_rate: 0.0010\n",
            "Epoch 7/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0564 - loss: 6.2523 - learning_rate: 0.0010\n",
            "Epoch 8/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.0627 - loss: 6.0697 - learning_rate: 0.0010\n",
            "Epoch 9/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.0689 - loss: 5.8896 - learning_rate: 0.0010\n",
            "Epoch 10/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0771 - loss: 5.6804 - learning_rate: 0.0010\n",
            "Epoch 11/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0870 - loss: 5.4677 - learning_rate: 0.0010\n",
            "Epoch 12/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.0969 - loss: 5.2931 - learning_rate: 0.0010\n",
            "Epoch 13/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.1015 - loss: 5.1189 - learning_rate: 0.0010\n",
            "Epoch 14/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.1083 - loss: 4.9338 - learning_rate: 0.0010\n",
            "Epoch 15/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.1218 - loss: 4.7198 - learning_rate: 0.0010\n",
            "Epoch 16/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.1411 - loss: 4.4997 - learning_rate: 0.0010\n",
            "Epoch 17/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.1591 - loss: 4.3702 - learning_rate: 0.0010\n",
            "Epoch 18/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.1815 - loss: 4.2142 - learning_rate: 0.0010\n",
            "Epoch 19/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.1872 - loss: 4.1294 - learning_rate: 0.0010\n",
            "Epoch 20/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.2159 - loss: 3.9464 - learning_rate: 0.0010\n",
            "Epoch 21/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.2395 - loss: 3.7679 - learning_rate: 0.0010\n",
            "Epoch 22/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.2612 - loss: 3.6312 - learning_rate: 0.0010\n",
            "Epoch 23/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.2829 - loss: 3.4815 - learning_rate: 0.0010\n",
            "Epoch 24/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.3017 - loss: 3.3630 - learning_rate: 0.0010\n",
            "Epoch 25/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.3187 - loss: 3.2325 - learning_rate: 0.0010\n",
            "Epoch 26/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.3435 - loss: 3.1072 - learning_rate: 0.0010\n",
            "Epoch 27/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.3553 - loss: 3.0070 - learning_rate: 0.0010\n",
            "Epoch 28/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.3724 - loss: 2.9016 - learning_rate: 0.0010\n",
            "Epoch 29/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.3790 - loss: 2.8324 - learning_rate: 0.0010\n",
            "Epoch 30/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.3936 - loss: 2.7494 - learning_rate: 0.0010\n",
            "Epoch 31/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.4054 - loss: 2.6833 - learning_rate: 0.0010\n",
            "Epoch 32/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4227 - loss: 2.5956 - learning_rate: 0.0010\n",
            "Epoch 33/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4226 - loss: 2.5624 - learning_rate: 0.0010\n",
            "Epoch 34/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.4378 - loss: 2.4786 - learning_rate: 0.0010\n",
            "Epoch 35/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4483 - loss: 2.4228 - learning_rate: 0.0010\n",
            "Epoch 36/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4479 - loss: 2.3995 - learning_rate: 0.0010\n",
            "Epoch 37/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4589 - loss: 2.3397 - learning_rate: 0.0010\n",
            "Epoch 38/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4711 - loss: 2.2825 - learning_rate: 0.0010\n",
            "Epoch 39/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.4772 - loss: 2.2545 - learning_rate: 0.0010\n",
            "Epoch 40/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.4864 - loss: 2.1936 - learning_rate: 0.0010\n",
            "Epoch 41/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4912 - loss: 2.1653 - learning_rate: 0.0010\n",
            "Epoch 42/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4937 - loss: 2.1430 - learning_rate: 0.0010\n",
            "Epoch 43/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5052 - loss: 2.0894 - learning_rate: 0.0010\n",
            "Epoch 44/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.5065 - loss: 2.0709 - learning_rate: 0.0010\n",
            "Epoch 45/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5147 - loss: 2.0217 - learning_rate: 0.0010\n",
            "Epoch 46/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5223 - loss: 1.9898 - learning_rate: 0.0010\n",
            "Epoch 47/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.5282 - loss: 1.9519 - learning_rate: 0.0010\n",
            "Epoch 48/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.5276 - loss: 1.9450 - learning_rate: 0.0010\n",
            "Epoch 49/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5298 - loss: 1.9185 - learning_rate: 0.0010\n",
            "Epoch 50/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5389 - loss: 1.8806 - learning_rate: 0.0010\n",
            "Epoch 51/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.5440 - loss: 1.8652 - learning_rate: 0.0010\n",
            "Epoch 52/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.5497 - loss: 1.8394 - learning_rate: 0.0010\n",
            "Epoch 53/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5550 - loss: 1.8087 - learning_rate: 0.0010\n",
            "Epoch 54/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5658 - loss: 1.7602 - learning_rate: 0.0010\n",
            "Epoch 55/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5633 - loss: 1.7596 - learning_rate: 0.0010\n",
            "Epoch 56/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5724 - loss: 1.7184 - learning_rate: 0.0010\n",
            "Epoch 57/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.5685 - loss: 1.7300 - learning_rate: 0.0010\n",
            "Epoch 58/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5780 - loss: 1.6727 - learning_rate: 0.0010\n",
            "Epoch 59/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.5806 - loss: 1.6679 - learning_rate: 0.0010\n",
            "Epoch 60/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5832 - loss: 1.6540 - learning_rate: 0.0010\n",
            "Epoch 61/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5866 - loss: 1.6313 - learning_rate: 0.0010\n",
            "Epoch 62/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.5924 - loss: 1.6036 - learning_rate: 0.0010\n",
            "Epoch 63/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5973 - loss: 1.5831 - learning_rate: 0.0010\n",
            "Epoch 64/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.5975 - loss: 1.5781 - learning_rate: 0.0010\n",
            "Epoch 65/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.6010 - loss: 1.5626 - learning_rate: 0.0010\n",
            "Epoch 66/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.6068 - loss: 1.5392 - learning_rate: 0.0010\n",
            "Epoch 67/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.6117 - loss: 1.5127 - learning_rate: 0.0010\n",
            "Epoch 68/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.6181 - loss: 1.4926 - learning_rate: 0.0010\n",
            "Epoch 69/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.6141 - loss: 1.4960 - learning_rate: 0.0010\n",
            "Epoch 70/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.6234 - loss: 1.4587 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 3 LSTM layers and 256 units per layer...\n",
            "Epoch 1/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - accuracy: 0.0211 - loss: 7.4836 - learning_rate: 0.0010\n",
            "Epoch 2/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.0225 - loss: 7.0660 - learning_rate: 0.0010\n",
            "Epoch 3/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.0242 - loss: 7.0133 - learning_rate: 0.0010\n",
            "Epoch 4/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.0262 - loss: 6.9584 - learning_rate: 0.0010\n",
            "Epoch 5/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.0325 - loss: 6.7401 - learning_rate: 0.0010\n",
            "Epoch 6/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.0437 - loss: 6.5002 - learning_rate: 0.0010\n",
            "Epoch 7/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.0509 - loss: 6.3099 - learning_rate: 0.0010\n",
            "Epoch 8/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.0587 - loss: 6.1132 - learning_rate: 0.0010\n",
            "Epoch 9/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.0709 - loss: 5.9086 - learning_rate: 0.0010\n",
            "Epoch 10/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.0785 - loss: 5.7090 - learning_rate: 0.0010\n",
            "Epoch 11/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.0882 - loss: 5.4826 - learning_rate: 0.0010\n",
            "Epoch 12/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.0986 - loss: 5.2537 - learning_rate: 0.0010\n",
            "Epoch 13/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.1078 - loss: 5.0066 - learning_rate: 0.0010\n",
            "Epoch 14/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.1199 - loss: 4.7326 - learning_rate: 0.0010\n",
            "Epoch 15/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.1372 - loss: 4.4771 - learning_rate: 0.0010\n",
            "Epoch 16/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.1580 - loss: 4.2339 - learning_rate: 0.0010\n",
            "Epoch 17/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.1938 - loss: 3.9520 - learning_rate: 0.0010\n",
            "Epoch 18/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.2300 - loss: 3.7247 - learning_rate: 0.0010\n",
            "Epoch 19/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.2684 - loss: 3.4694 - learning_rate: 0.0010\n",
            "Epoch 20/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.3047 - loss: 3.2420 - learning_rate: 0.0010\n",
            "Epoch 21/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.3330 - loss: 3.0505 - learning_rate: 0.0010\n",
            "Epoch 22/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.3680 - loss: 2.8300 - learning_rate: 0.0010\n",
            "Epoch 23/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.3916 - loss: 2.6883 - learning_rate: 0.0010\n",
            "Epoch 24/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.4214 - loss: 2.5177 - learning_rate: 0.0010\n",
            "Epoch 25/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.4409 - loss: 2.3942 - learning_rate: 0.0010\n",
            "Epoch 26/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.4648 - loss: 2.2810 - learning_rate: 0.0010\n",
            "Epoch 27/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.4819 - loss: 2.1765 - learning_rate: 0.0010\n",
            "Epoch 28/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5027 - loss: 2.0570 - learning_rate: 0.0010\n",
            "Epoch 29/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5238 - loss: 1.9610 - learning_rate: 0.0010\n",
            "Epoch 30/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5375 - loss: 1.8828 - learning_rate: 0.0010\n",
            "Epoch 31/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5543 - loss: 1.7908 - learning_rate: 0.0010\n",
            "Epoch 32/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.5709 - loss: 1.7113 - learning_rate: 0.0010\n",
            "Epoch 33/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5882 - loss: 1.6207 - learning_rate: 0.0010\n",
            "Epoch 34/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.6030 - loss: 1.5505 - learning_rate: 0.0010\n",
            "Epoch 35/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.6172 - loss: 1.4855 - learning_rate: 0.0010\n",
            "Epoch 36/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - accuracy: 0.6312 - loss: 1.4374 - learning_rate: 0.0010\n",
            "Epoch 37/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.6402 - loss: 1.3867 - learning_rate: 0.0010\n",
            "Epoch 38/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.6526 - loss: 1.3179 - learning_rate: 0.0010\n",
            "Epoch 39/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.6699 - loss: 1.2551 - learning_rate: 0.0010\n",
            "Epoch 40/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.6784 - loss: 1.2073 - learning_rate: 0.0010\n",
            "Epoch 41/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.6924 - loss: 1.1530 - learning_rate: 0.0010\n",
            "Epoch 42/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.6999 - loss: 1.1094 - learning_rate: 0.0010\n",
            "Epoch 43/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.7168 - loss: 1.0378 - learning_rate: 0.0010\n",
            "Epoch 44/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.7260 - loss: 1.0057 - learning_rate: 0.0010\n",
            "Epoch 45/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.7298 - loss: 0.9765 - learning_rate: 0.0010\n",
            "Epoch 46/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.7450 - loss: 0.9252 - learning_rate: 0.0010\n",
            "Epoch 47/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.7487 - loss: 0.8974 - learning_rate: 0.0010\n",
            "Epoch 48/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.7626 - loss: 0.8436 - learning_rate: 0.0010\n",
            "Epoch 49/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.7689 - loss: 0.8224 - learning_rate: 0.0010\n",
            "Epoch 50/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - accuracy: 0.7818 - loss: 0.7705 - learning_rate: 0.0010\n",
            "Epoch 51/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.7920 - loss: 0.7309 - learning_rate: 0.0010\n",
            "Epoch 52/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - accuracy: 0.7952 - loss: 0.7142 - learning_rate: 0.0010\n",
            "Epoch 53/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8028 - loss: 0.6780 - learning_rate: 0.0010\n",
            "Epoch 54/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.8089 - loss: 0.6592 - learning_rate: 0.0010\n",
            "Epoch 55/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8182 - loss: 0.6263 - learning_rate: 0.0010\n",
            "Epoch 56/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.8266 - loss: 0.5915 - learning_rate: 0.0010\n",
            "Epoch 57/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8386 - loss: 0.5559 - learning_rate: 0.0010\n",
            "Epoch 58/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8397 - loss: 0.5479 - learning_rate: 0.0010\n",
            "Epoch 59/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8423 - loss: 0.5322 - learning_rate: 0.0010\n",
            "Epoch 60/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.8495 - loss: 0.5053 - learning_rate: 0.0010\n",
            "Epoch 61/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.8579 - loss: 0.4800 - learning_rate: 0.0010\n",
            "Epoch 62/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 55ms/step - accuracy: 0.8527 - loss: 0.4941 - learning_rate: 0.0010\n",
            "Epoch 63/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8691 - loss: 0.4446 - learning_rate: 0.0010\n",
            "Epoch 64/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8747 - loss: 0.4181 - learning_rate: 0.0010\n",
            "Epoch 65/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8775 - loss: 0.4135 - learning_rate: 0.0010\n",
            "Epoch 66/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8783 - loss: 0.4012 - learning_rate: 0.0010\n",
            "Epoch 67/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8838 - loss: 0.3879 - learning_rate: 0.0010\n",
            "Epoch 68/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.8870 - loss: 0.3694 - learning_rate: 0.0010\n",
            "Epoch 69/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.8849 - loss: 0.3789 - learning_rate: 0.0010\n",
            "Epoch 70/70\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.8924 - loss: 0.3575 - learning_rate: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, temperature=1.0, num_words=50):\n",
        "    result = []\n",
        "    input_text = seed_text\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')\n",
        "\n",
        "        predictions = model.predict(token_list, verbose=0)[0]\n",
        "        predictions = np.log(predictions + 1e-10) / temperature\n",
        "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "\n",
        "        predicted_word_index = np.random.choice(range(vocab_size), p=predictions)\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
        "\n",
        "        input_text += \" \" + predicted_word\n",
        "        result.append(predicted_word)\n",
        "\n",
        "    return \" \".join(result)\n"
      ],
      "metadata": {
        "id": "E-lkK16vUPHk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\"to be or not to be\", \"shall i compare thee to a summer's day\", \"all the world's a stage\"]\n",
        "temperatures = [0.1, 0.5, 1.0]\n",
        "\n",
        "for model_name, model, _ in models_histories:\n",
        "    print(f\"\\n--- Evaluating Model: {model_name} ---\")\n",
        "    for temp in temperatures:\n",
        "        for prompt in prompts:\n",
        "            print(f\"\\nPrompt: '{prompt}' | Temperature: {temp}\")\n",
        "            generated_text = generate_text(model, tokenizer, prompt, temperature=temp)\n",
        "            print(f\"Generated Text: {generated_text}\")\n",
        "            print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpI7yfaEUXNS",
        "outputId": "82f478cd-4392-4d0c-af05-26e39e615d3e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Model: 1 layers, 64 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " whoever hath\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " to\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " killing that loue thou hast\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " or\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " all tongues that subject before the days watch in time\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " heare the world may be here i doubt not all with ere it go to field be in prepare her to\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " \n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " for\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " but today\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 1 layers, 128 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " rom if i speake\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " rom o\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " be\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " out\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " iul he have i doe you with twenty of this foote i will not keepe how holy names are sent \n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " why well is power\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " from thyself the world do come so\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " ro tis the way\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " iul i haue fought in the part of day an vnaccustomd backe made me such close ile liue to go to\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 1 layers, 256 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " so will i\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " it is the star to every\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " to\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " fri o when i do forget\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " they made false wormesmeat\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " meta charsetutf8 title404 project gutenbergtitle link relstylesheet hrefgutenbergstylecssv11 link relstylesheet hrefgutenbergcollapsiblecss11 link relstylesheet hrefgutenbergnewnavcssv1321231 link relstylesheet hrefgutenbergpgdesktoponecss meta nameviewport contentwidthdevicewidth initialscale1 meta namekeywords contentbooks ebooks free kindle android iphone ipad meta namegooglesiteverification contentwucoevsnj5kp3ts36ofp64laakk1mvtgptrgc9io meta namealexaverifyid content4wnacljsea82vpih2huqxzvm link relcopyright hrefhttpswwwgnuorgcopyleftfdlhtml link\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " to\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " no life but thine is not aduanced\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " therefore that\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 2 layers, 64 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " than die for\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " that i\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " \n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " or yet he may no romeo\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " and like more enemies with a poets buds and\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " iul come knows\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " but stop for a\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " who rust and should me absent\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " nor it as she be stay\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 2 layers, 128 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " iul well she is well iuliet madam\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " praise ile lay lest i am so\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " for king nor i i see him with these\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " thou hast my true\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " till indeed is flower with base infection meet\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " \n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " and play the earth if that he look so\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " shall with strong most did bright say once thee too\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " a body man in my dull all\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 2 layers, 256 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " they wilt\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " as used thought an enjoyer and golden time\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " lady is by their voice should tell thee ioyfull daughter wheres my heart be power and rough abouts heauen but all\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " my love hath not no old leisure gave his one by succession\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " the which a baud is\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " rom to the maister here or you\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " the heauen do if the two\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " that thou in all a\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " our bridall flowers serue now that my antique auncestors him vanishing for be but on\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 3 layers, 64 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " the jewel of\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " those art thy love to lovely yourself to lovely varying by thy glass are given give\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " than to both to mine owne\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " greg thats mongers that i\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " o is thy\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " i\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " but moving is from a cheek which time no\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " but rank will let her poets concord or faults and lovely though and they shall eyes\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " new\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 3 layers, 128 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " on the teeme of little atomies\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " though\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " par if do can giue call\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " rom didst do\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " and others be written on thy\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " rom draw i am coarse\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " and canst thou mad say night whom\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " if a rank poyson of streakes a time my eyes for draue at\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " thus in the world that\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 3 layers, 256 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " mer my heart is can a man come as thou had\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " we\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " whilst\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " and as he summers best shall\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " take where i saw thee so one to day is but to romeo bring no my\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " am and you romeo still loue bold other graue if that my owne right are a my\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " my daughters on\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " at art here cozin then now with banishment\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " it far audit love that\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}