{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alanazi433/GenAI/blob/main/GenAI/HW5/Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEzF5CS0dNnF",
        "outputId": "050454ff-ab57-4dd3-f612-7790a57261d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import re\n",
        "import string\n"
      ],
      "metadata": {
        "id": "ShH4-Wh_f-yg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Download and combine text data\n",
        "urls = [\n",
        "\"https://www.gutenberg.org/files/1041/1041-0.txt\",  # Hamlet\n",
        "    \"https://www.gutenberg.org/files/152/152-0.txt\",    # Macbeth\n",
        "    \"https://www.gutenberg.org/files/1112/1112-0.txt\",  # Othello\n",
        "    \"https://www.gutenberg.org/files/2265/2265-0.txt\",  # A Midsummer Night's Dream\n",
        "    \"https://www.gutenberg.org/files/1787/1787-0.txt\",  # King Lear\n",
        "    \"https://www.gutenberg.org/files/2264/2264-0.txt\"   # The Tempest\n",
        "]\n",
        "\n",
        "all_text = \"\"\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    all_text += response.text + \"\\n\\n\"\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "all_text = clean_text(all_text)\n",
        "\n",
        "# Tokenize and create sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([all_text])\n",
        "sequences = tokenizer.texts_to_sequences([all_text])[0]\n",
        "sequence_length = 100\n",
        "\n",
        "# Create input (X) and target (y) sequences\n",
        "sequences = [sequences[i:i + sequence_length + 1] for i in range(len(sequences) - sequence_length)]\n",
        "X, y = [], []\n",
        "for seq in sequences:\n",
        "    X.append(seq[:-1])\n",
        "    y.append(seq[-1])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n"
      ],
      "metadata": {
        "id": "lFEjoVvOdZLE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(num_layers, units, epochs=50):\n",
        "    inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "    x = layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "\n",
        "    # Add LSTM layers with dropout for regularization\n",
        "    for _ in range(num_layers - 1):\n",
        "        x = layers.LSTM(units, return_sequences=True, dropout=0.4)(x)\n",
        "    x = layers.LSTM(units, dropout=0.4)(x)\n",
        "\n",
        "    # Layer normalization\n",
        "    x = layers.LayerNormalization()(x)\n",
        "\n",
        "    outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "    model = models.Model(inputs, outputs)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Learning rate scheduler for smoother convergence\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    # Train with modified batch size or increased sequence length\n",
        "    history = model.fit(X, y, batch_size=128, epochs=epochs, callbacks=[lr_scheduler])\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Configurations for models with different layers and units\n",
        "configurations = [\n",
        "    {\"num_layers\": 1, \"units\": 64},\n",
        "    {\"num_layers\": 1, \"units\": 128},\n",
        "    {\"num_layers\": 1, \"units\": 256},\n",
        "    {\"num_layers\": 2, \"units\": 64},\n",
        "    {\"num_layers\": 2, \"units\": 128},\n",
        "    {\"num_layers\": 2, \"units\": 256},\n",
        "    {\"num_layers\": 3, \"units\": 64},\n",
        "    {\"num_layers\": 3, \"units\": 128},\n",
        "    {\"num_layers\": 3, \"units\": 256}\n",
        "]\n",
        "\n",
        "models_histories = []\n",
        "for config in configurations:\n",
        "    print(f\"\\nTraining model with {config['num_layers']} LSTM layers and {config['units']} units per layer...\")\n",
        "    model, history = train_and_evaluate_model(config['num_layers'], config['units'])\n",
        "    models_histories.append((f\"{config['num_layers']} layers, {config['units']} units\", model, history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tYTzOYEdcRT",
        "outputId": "4367fd71-a249-46e7-afc0-3f512aecf53c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with 1 LSTM layers and 64 units per layer...\n",
            "Epoch 1/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.0209 - loss: 7.6101 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.0399 - loss: 6.7383 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.0814 - loss: 6.1876 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.1194 - loss: 5.6264 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.1530 - loss: 5.0945 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.1957 - loss: 4.5723 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2382 - loss: 4.0991 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.2904 - loss: 3.6487 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.3474 - loss: 3.2424 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.4053 - loss: 2.8845 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.4572 - loss: 2.5791 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.4996 - loss: 2.3263 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5199 - loss: 2.1611 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5488 - loss: 1.9929 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.5687 - loss: 1.8642 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5906 - loss: 1.7485 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6044 - loss: 1.6684 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6179 - loss: 1.5988 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6270 - loss: 1.5220 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6407 - loss: 1.4560 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.6483 - loss: 1.4064 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6600 - loss: 1.3520 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.6703 - loss: 1.3008 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6701 - loss: 1.2840 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.6839 - loss: 1.2307 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6838 - loss: 1.2156 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6973 - loss: 1.1643 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6928 - loss: 1.1660 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.6956 - loss: 1.1507 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7055 - loss: 1.1053 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7062 - loss: 1.0929 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7117 - loss: 1.0729 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.7198 - loss: 1.0403 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7164 - loss: 1.0421 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7249 - loss: 1.0139 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7254 - loss: 1.0023 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7299 - loss: 0.9937 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.7391 - loss: 0.9628 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7415 - loss: 0.9416 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7393 - loss: 0.9421 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7397 - loss: 0.9453 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7442 - loss: 0.9163 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7467 - loss: 0.9105 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7490 - loss: 0.9015 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.7485 - loss: 0.9011 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7500 - loss: 0.8927 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7511 - loss: 0.8844 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7551 - loss: 0.8684 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7604 - loss: 0.8612 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7598 - loss: 0.8467 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 1 LSTM layers and 128 units per layer...\n",
            "Epoch 1/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.0225 - loss: 7.4957 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.0408 - loss: 6.7151 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.0810 - loss: 6.0969 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1260 - loss: 5.4110 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.1698 - loss: 4.7652 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.2214 - loss: 4.1441 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.3042 - loss: 3.5290 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.3796 - loss: 2.9882 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.4619 - loss: 2.5196 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5056 - loss: 2.2143 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5513 - loss: 1.9590 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5834 - loss: 1.7629 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6105 - loss: 1.6111 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6399 - loss: 1.4709 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6567 - loss: 1.3657 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.6785 - loss: 1.2592 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.6916 - loss: 1.1988 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7085 - loss: 1.1216 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7169 - loss: 1.0682 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.7311 - loss: 1.0071 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7373 - loss: 0.9763 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7507 - loss: 0.9154 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7606 - loss: 0.8712 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7629 - loss: 0.8566 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7756 - loss: 0.8008 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7759 - loss: 0.7954 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7864 - loss: 0.7592 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7904 - loss: 0.7437 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7981 - loss: 0.7069 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7996 - loss: 0.6963 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8075 - loss: 0.6773 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8109 - loss: 0.6534 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8205 - loss: 0.6258 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8217 - loss: 0.6105 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8252 - loss: 0.5998 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8251 - loss: 0.5916 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8296 - loss: 0.5820 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8347 - loss: 0.5638 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8344 - loss: 0.5616 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8337 - loss: 0.5565 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8397 - loss: 0.5416 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8468 - loss: 0.5234 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8451 - loss: 0.5164 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8431 - loss: 0.5237 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8542 - loss: 0.4841 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8557 - loss: 0.4855 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8538 - loss: 0.4839 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8541 - loss: 0.4808 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8580 - loss: 0.4732 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8590 - loss: 0.4704 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 1 LSTM layers and 256 units per layer...\n",
            "Epoch 1/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.0217 - loss: 7.5058 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.0264 - loss: 6.9685 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0438 - loss: 6.6342 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0585 - loss: 6.3272 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.0777 - loss: 6.0103 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0963 - loss: 5.6850 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1088 - loss: 5.3422 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.1330 - loss: 4.9322 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1611 - loss: 4.5072 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.2018 - loss: 4.0781 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.2608 - loss: 3.6815 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.3263 - loss: 3.2381 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.3840 - loss: 2.8808 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4378 - loss: 2.5599 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.4902 - loss: 2.2752 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.5335 - loss: 2.0374 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5721 - loss: 1.8263 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6130 - loss: 1.6331 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.6397 - loss: 1.4830 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.6716 - loss: 1.3353 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.7011 - loss: 1.2025 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.7308 - loss: 1.0802 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.7483 - loss: 0.9795 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.7579 - loss: 0.9236 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.7877 - loss: 0.8045 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8004 - loss: 0.7506 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.8201 - loss: 0.6677 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.8278 - loss: 0.6307 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.8433 - loss: 0.5630 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8526 - loss: 0.5272 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.8623 - loss: 0.4844 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8768 - loss: 0.4383 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.8775 - loss: 0.4207 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.8916 - loss: 0.3797 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8911 - loss: 0.3747 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9001 - loss: 0.3385 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9069 - loss: 0.3157 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9079 - loss: 0.3107 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9177 - loss: 0.2812 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9147 - loss: 0.2868 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9172 - loss: 0.2759 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9236 - loss: 0.2551 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9228 - loss: 0.2517 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9317 - loss: 0.2272 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9316 - loss: 0.2273 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9350 - loss: 0.2184 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9332 - loss: 0.2200 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9391 - loss: 0.2049 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9322 - loss: 0.2171 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9408 - loss: 0.1960 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 2 LSTM layers and 64 units per layer...\n",
            "Epoch 1/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.0210 - loss: 7.5917 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.0287 - loss: 6.9043 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.0433 - loss: 6.6234 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.0545 - loss: 6.3760 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.0660 - loss: 6.0815 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.0731 - loss: 5.8450 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.0815 - loss: 5.5995 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.0955 - loss: 5.3692 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.1076 - loss: 5.1316 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.1209 - loss: 4.9201 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.1360 - loss: 4.6877 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.1556 - loss: 4.4925 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.1760 - loss: 4.3192 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.1970 - loss: 4.1407 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.2172 - loss: 3.9764 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.2398 - loss: 3.8400 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.2599 - loss: 3.6798 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.2735 - loss: 3.5688 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.2915 - loss: 3.4615 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.3070 - loss: 3.3454 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.3177 - loss: 3.2653 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.3289 - loss: 3.1896 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.3386 - loss: 3.1060 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.3541 - loss: 3.0155 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.3557 - loss: 2.9801 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3654 - loss: 2.9278 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.3709 - loss: 2.8613 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.3825 - loss: 2.8006 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.3920 - loss: 2.7398 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.3971 - loss: 2.7128 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.3986 - loss: 2.6763 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4076 - loss: 2.6444 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4146 - loss: 2.5967 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4144 - loss: 2.5714 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4242 - loss: 2.5291 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4262 - loss: 2.5174 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4341 - loss: 2.4641 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4350 - loss: 2.4539 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4426 - loss: 2.4219 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4414 - loss: 2.3975 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4486 - loss: 2.3758 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.4520 - loss: 2.3613 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.4615 - loss: 2.3197 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.4568 - loss: 2.3243 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4671 - loss: 2.2697 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4598 - loss: 2.2722 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.4660 - loss: 2.2452 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4747 - loss: 2.2211 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.4761 - loss: 2.2079 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.4760 - loss: 2.2045 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 2 LSTM layers and 128 units per layer...\n",
            "Epoch 1/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.0205 - loss: 7.5307 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0255 - loss: 6.9948 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0340 - loss: 6.7733 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0445 - loss: 6.5837 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0555 - loss: 6.3516 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0616 - loss: 6.1663 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0696 - loss: 5.9843 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0777 - loss: 5.7663 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0902 - loss: 5.5109 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0986 - loss: 5.2733 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1051 - loss: 5.0709 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.1182 - loss: 4.8169 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.1351 - loss: 4.6039 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.1539 - loss: 4.3962 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.1711 - loss: 4.2096 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1958 - loss: 4.0279 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.2198 - loss: 3.8760 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.2387 - loss: 3.7119 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.2639 - loss: 3.5450 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.2776 - loss: 3.4424 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.2913 - loss: 3.3238 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3140 - loss: 3.2026 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3298 - loss: 3.0976 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.3428 - loss: 2.9800 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3551 - loss: 2.9281 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3723 - loss: 2.8368 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3858 - loss: 2.7400 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3892 - loss: 2.6971 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4027 - loss: 2.6041 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4108 - loss: 2.5628 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4202 - loss: 2.5102 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4329 - loss: 2.4441 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4396 - loss: 2.4000 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4435 - loss: 2.3754 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.4475 - loss: 2.3354 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4624 - loss: 2.2711 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4648 - loss: 2.2538 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4742 - loss: 2.1934 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4817 - loss: 2.1494 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.4888 - loss: 2.1198 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4917 - loss: 2.0991 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4940 - loss: 2.0811 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.5009 - loss: 2.0284 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5184 - loss: 1.9656 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.5174 - loss: 1.9670 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5222 - loss: 1.9357 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.5233 - loss: 1.9156 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.5310 - loss: 1.8935 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5363 - loss: 1.8451 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.5463 - loss: 1.8220 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 2 LSTM layers and 256 units per layer...\n",
            "Epoch 1/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 38ms/step - accuracy: 0.0233 - loss: 7.4818 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.0237 - loss: 7.0834 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.0292 - loss: 6.8486 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.0419 - loss: 6.6577 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.0477 - loss: 6.4324 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.0566 - loss: 6.2493 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.0655 - loss: 6.0449 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.0718 - loss: 5.8538 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.0844 - loss: 5.6329 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.0890 - loss: 5.4333 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.1029 - loss: 5.1782 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.1118 - loss: 4.9472 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.1274 - loss: 4.6982 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.1474 - loss: 4.4329 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.1691 - loss: 4.2022 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.2007 - loss: 3.9571 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.2342 - loss: 3.7085 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.2602 - loss: 3.4959 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.2901 - loss: 3.3016 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.3262 - loss: 3.0980 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.3508 - loss: 2.9377 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - accuracy: 0.3735 - loss: 2.7918 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.3969 - loss: 2.6555 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - accuracy: 0.4182 - loss: 2.5312 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.4328 - loss: 2.4445 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.4588 - loss: 2.3284 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.4740 - loss: 2.2311 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.4835 - loss: 2.1678 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.5045 - loss: 2.0702 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.5142 - loss: 1.9978 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 39ms/step - accuracy: 0.5285 - loss: 1.9283 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.5434 - loss: 1.8639 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.5532 - loss: 1.8050 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.5675 - loss: 1.7353 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.5755 - loss: 1.6934 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.5855 - loss: 1.6439 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.5968 - loss: 1.5862 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.6043 - loss: 1.5401 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.6154 - loss: 1.4946 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.6265 - loss: 1.4441 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.6353 - loss: 1.4025 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.6434 - loss: 1.3666 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.6538 - loss: 1.3200 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.6615 - loss: 1.2923 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.6611 - loss: 1.2786 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.6760 - loss: 1.2190 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.6832 - loss: 1.1859 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.6897 - loss: 1.1508 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.7000 - loss: 1.1199 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.6979 - loss: 1.1041 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 3 LSTM layers and 64 units per layer...\n",
            "Epoch 1/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0214 - loss: 7.5872 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0225 - loss: 6.9798 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0269 - loss: 6.9629 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.0266 - loss: 6.8965 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0416 - loss: 6.6455 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.0500 - loss: 6.4495 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.0577 - loss: 6.2707 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0637 - loss: 6.1180 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.0683 - loss: 5.9472 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.0726 - loss: 5.7742 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0794 - loss: 5.6094 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.0808 - loss: 5.4743 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.0849 - loss: 5.3384 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.0885 - loss: 5.1990 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.0954 - loss: 5.0744 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1053 - loss: 4.9280 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.1132 - loss: 4.8111 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.1209 - loss: 4.6769 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.1328 - loss: 4.5506 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.1462 - loss: 4.4298 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.1568 - loss: 4.3206 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.1744 - loss: 4.2143 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1889 - loss: 4.0822 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.2045 - loss: 3.9815 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.2164 - loss: 3.9066 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.2317 - loss: 3.8031 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.2434 - loss: 3.7146 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.2541 - loss: 3.6426 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.2635 - loss: 3.5823 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.2764 - loss: 3.4906 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.2820 - loss: 3.4336 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.2883 - loss: 3.3792 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.2950 - loss: 3.3291 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3082 - loss: 3.2660 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.3067 - loss: 3.2365 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.3167 - loss: 3.1738 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.3259 - loss: 3.1394 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.3259 - loss: 3.1022 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.3389 - loss: 3.0597 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3439 - loss: 3.0129 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.3453 - loss: 2.9956 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3482 - loss: 2.9801 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3582 - loss: 2.9232 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.3544 - loss: 2.9151 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3592 - loss: 2.8957 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3663 - loss: 2.8478 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.3634 - loss: 2.8379 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.3743 - loss: 2.7994 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.3738 - loss: 2.7778 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.3816 - loss: 2.7506 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 3 LSTM layers and 128 units per layer...\n",
            "Epoch 1/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - accuracy: 0.0221 - loss: 7.5133 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0222 - loss: 7.0180 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0260 - loss: 6.9745 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0262 - loss: 6.8568 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.0349 - loss: 6.6693 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0473 - loss: 6.4628 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.0541 - loss: 6.2768 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.0657 - loss: 6.0988 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.0753 - loss: 5.8638 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.0798 - loss: 5.6621 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.0893 - loss: 5.4286 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.0981 - loss: 5.1844 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.1076 - loss: 4.9501 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.1179 - loss: 4.7377 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.1358 - loss: 4.5210 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.1647 - loss: 4.2995 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.1896 - loss: 4.1149 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.2130 - loss: 3.9244 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.2369 - loss: 3.7635 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.2586 - loss: 3.6178 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.2821 - loss: 3.4783 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.3013 - loss: 3.3567 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.3149 - loss: 3.2296 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.3257 - loss: 3.1556 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.3401 - loss: 3.0573 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.3543 - loss: 2.9562 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.3691 - loss: 2.8771 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.3784 - loss: 2.8014 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.3936 - loss: 2.7185 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.3990 - loss: 2.6667 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4128 - loss: 2.6020 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.4213 - loss: 2.5398 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.4266 - loss: 2.4958 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.4444 - loss: 2.4267 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.4466 - loss: 2.3869 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4553 - loss: 2.3405 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.4612 - loss: 2.2859 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.4654 - loss: 2.2606 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.4764 - loss: 2.2238 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.4840 - loss: 2.1714 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.4882 - loss: 2.1391 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.4974 - loss: 2.0982 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.4960 - loss: 2.0929 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.5018 - loss: 2.0552 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.5093 - loss: 2.0113 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.5157 - loss: 1.9821 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.5208 - loss: 1.9556 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.5232 - loss: 1.9429 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.5355 - loss: 1.8968 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.5334 - loss: 1.8801 - learning_rate: 0.0010\n",
            "\n",
            "Training model with 3 LSTM layers and 256 units per layer...\n",
            "Epoch 1/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 57ms/step - accuracy: 0.0205 - loss: 7.4959 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0227 - loss: 7.0867 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0232 - loss: 7.0003 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0237 - loss: 6.9377 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.0336 - loss: 6.7467 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0431 - loss: 6.5186 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0526 - loss: 6.2996 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0634 - loss: 6.0936 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0753 - loss: 5.9033 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0870 - loss: 5.6914 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0872 - loss: 5.5540 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.1024 - loss: 5.2528 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.1139 - loss: 5.0042 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.1248 - loss: 4.7304 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.1487 - loss: 4.4297 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.1843 - loss: 4.1182 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.2246 - loss: 3.8336 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.2627 - loss: 3.5682 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.2950 - loss: 3.3340 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.3287 - loss: 3.1202 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.3607 - loss: 2.9008 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.3846 - loss: 2.7556 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.4104 - loss: 2.5895 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.4373 - loss: 2.4356 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.4627 - loss: 2.2962 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.4832 - loss: 2.1853 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.5016 - loss: 2.0820 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.5185 - loss: 1.9862 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.5380 - loss: 1.8844 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.5524 - loss: 1.8182 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.5676 - loss: 1.7268 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.5864 - loss: 1.6338 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.6028 - loss: 1.5624 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.6075 - loss: 1.5139 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.6241 - loss: 1.4478 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.6442 - loss: 1.3593 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.6518 - loss: 1.3055 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.6643 - loss: 1.2488 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.6850 - loss: 1.1797 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.6941 - loss: 1.1322 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.7132 - loss: 1.0628 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.7189 - loss: 1.0382 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.7309 - loss: 0.9841 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 57ms/step - accuracy: 0.7356 - loss: 0.9583 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 57ms/step - accuracy: 0.7522 - loss: 0.8920 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.7634 - loss: 0.8485 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 57ms/step - accuracy: 0.7731 - loss: 0.8056 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.7776 - loss: 0.7893 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 57ms/step - accuracy: 0.7904 - loss: 0.7396 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.8001 - loss: 0.7016 - learning_rate: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, temperature=1.0, num_words=50):\n",
        "    result = []\n",
        "    input_text = seed_text\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')\n",
        "\n",
        "        predictions = model.predict(token_list, verbose=0)[0]\n",
        "        predictions = np.log(predictions + 1e-10) / temperature\n",
        "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "\n",
        "        predicted_word_index = np.random.choice(range(vocab_size), p=predictions)\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
        "\n",
        "        input_text += \" \" + predicted_word\n",
        "        result.append(predicted_word)\n",
        "\n",
        "    return \" \".join(result)\n"
      ],
      "metadata": {
        "id": "E-lkK16vUPHk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\"to be or not to be\", \"shall i compare thee to a summer's day\", \"all the world's a stage\"]\n",
        "temperatures = [0.1, 0.5, 1.0]\n",
        "\n",
        "for model_name, model, _ in models_histories:\n",
        "    print(f\"\\n--- Evaluating Model: {model_name} ---\")\n",
        "    for temp in temperatures:\n",
        "        for prompt in prompts:\n",
        "            print(f\"\\nPrompt: '{prompt}' | Temperature: {temp}\")\n",
        "            generated_text = generate_text(model, tokenizer, prompt, temperature=temp)\n",
        "            print(f\"Generated Text: {generated_text}\")\n",
        "            print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpI7yfaEUXNS",
        "outputId": "761243bb-0bdc-4488-e2d7-9d623807858d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Model: 1 layers, 64 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " if thou answer not bounteous gift\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " which all in these fellow\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " what canst in thou child done shall whom thou hath not budge for no mans mercy bid these lie with her thing to my dog of the classfooter or they peace vp fortune of such triumph and marrie how he should every blowes vs old\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " iul loving thoughts upon thy love that i charge thee ere thou in hearst thou seest all the roses do\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " that first days\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " thou conscience is this of\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " and you can wretched wanting with the worse to moue was a\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " thus or shall be fast i warrant you\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " and they for have\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 1 layers, 128 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " this\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " tib this feare is well shall testifie\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " thy consent to night is\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " rom by my glass and we shall\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " rom o vncle capulet and his\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " and dost\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " one for\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " rom i sell thee poyson\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " i on old black\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 1 layers, 256 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " what say the heauens to this say i come heere to\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " come waken may knowes you whats the day\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " my bones and like a forme cries with thy old brawles doth lie a beautious flower when a sun\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " iul walke in thee of my\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " now\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " mer god will follow thee him for you shall meete\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " iul i\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " rom\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " and now this faire\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 2 layers, 64 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " and then noble sweare to speak\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " that i thanke you liue to liue to shift\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " as pale as seeming she would would doe i question whateer the\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " because\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " rom\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " to\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " till that the poultis which\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " and what shalt\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " o as the the proud\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 2 layers, 128 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " and i no earth i needs not prove be consorted to the world of\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " which hath not the pretty\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " to\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " and\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " may who i do then here\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " iul o teach me bury sake\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " tired better to my\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " let this would wink i\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " since this life\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 2 layers, 256 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " receiving nought with elements a\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " and\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " nur i gaue thee\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " and then was my heart and i assure ye\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " and that is mine whose time may thinke\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " by chance and natures\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " makes summers lives th executor being earth with\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " a idmainlogo href classnohover were your first is tell me for it outward quarrell new night not robs and and are very poor child in both that sin by the old bench that was profand nature i not dost\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " and by the time that keeps although with\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 3 layers, 64 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " mer i will not\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " to this alike that\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " madam yes\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " rom this\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " of lady ought odde thou ioy not crosse a sword\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " iul peace brother inough i am a falkners vpon this lure\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " the thousand inough eates for the life o lady we men\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " for\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " either what hap see i go vp very\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 3 layers, 128 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " and if thou thinkest that there will make the\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " cap i saw\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " nur\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " or what dost do to see it i now not\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " yet yet thou art die in their minute count my love failes \n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " iul and\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " o to church for mens nurse\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " \n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " against that time thou shalt hundred griefes of best\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Evaluating Model: 3 layers, 256 units ---\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.1\n",
            " as is\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.1\n",
            " romeo for his heele is i will but out from their graue then he that then out to iocond\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.1\n",
            " rom o comfortable frier wheres my flesh i shall giue you o for the rest of your ioy\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 0.5\n",
            " iul nurse thou shall giue thee they consent some loue but but when thou art not leaue me not i will\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 0.5\n",
            " iul\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 0.5\n",
            " par\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'to be or not to be' | Temperature: 1.0\n",
            " morsell away\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'shall i compare thee to a summer's day' | Temperature: 1.0\n",
            " and they do they were common\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: 'all the world's a stage' | Temperature: 1.0\n",
            " and then thou wilt yet i will call the losse in noble parentage\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}